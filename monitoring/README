sudo apt install jsonnet
sudo apt install golang-go
GO111MODULE="on" go get github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb
GO111MODULE="on" go get github.com/brancz/gojsontoyaml
export PATH=$PATH:$(go env GOPATH)/bin

mkdir my-kube-prometheus; cd my-kube-prometheus
jb init
# latest release as of 15 12. 2021
jb install github.com/prometheus-operator/kube-prometheus/jsonnet/kube-prometheus@release-0.9
wget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/release-0.9/example.jsonnet -O example.jsonnet
wget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/release-0.9/build.sh -O build.sh

wget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/examples/prometheus-pvc.jsonnet

edit prometheus-pvc.jsonnet
# retention: '1y'
# storageClassName: 'nfs-monitoring',                                                                  




################# selector: { matchLabels: { 'app.kubernetes.io/name': 'monitoring' } },                 

###bash build.sh example.jsonnet
bash build.sh prometheus-pvc.jsonnet

# érdemes a namespace objektumot eldobni, mert azt már kézzel megcsináltuk
rm manifests/setup/0namespace-namespace.yaml

kubectl create -f manifests/setup
# unable to recognize "manifests/setup/0namespace-prometheusRule.yaml": no matches for kind "PrometheusRule" in version "monitoring.coreos.com/v1"
# unable to recognize "manifests/setup/prometheus-operator-prometheusRule.yaml": no matches for kind "PrometheusRule" in version "monitoring.coreos.com/v1"
# wait for a sec
kubectl create -f manifests/setup


kubectl get servicemonitors --all-namespaces
# No resources found...

kubectl get pods -n monitoring
# prometheus-operator-XXXXXXXXXX-XXXXX   2/2     Running   0          56s

kubectl logs -n monitoring prometheus-operator-XXXXXXXXXX-XXXXX -c prometheus-operator
# ...
# level=info ts=2021-02-10T13:14:54.989280762Z caller=operator.go:279 component=alertmanageroperator msg="successfully synced all caches"

kubectl logs -f -n monitoring prometheus-operator-XXXXXXXXXX-XXXXX -c kube-rbac-proxy
# ...
# I0210 14:54:57.554140       1 main.go:318] Listening securely on :8443


kubectl create -f manifests/
# minden ok

kubectl get servicemonitors --all-namespaces
##############
# NAMESPACE    NAME                      AGE
# monitoring   alertmanager              31s
# monitoring   blackbox-exporter         31s
# monitoring   coredns                   27s
# monitoring   grafana                   29s
# monitoring   kube-apiserver            27s
# monitoring   kube-controller-manager   27s
# monitoring   kube-scheduler            27s
# monitoring   kube-state-metrics        29s
# monitoring   kubelet                   27s
# monitoring   node-exporter             29s
# monitoring   prometheus                28s
# monitoring   prometheus-adapter        28s
# monitoring   prometheus-operator       28s
##############

kubectl get svc -n monitoring
# NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
# alertmanager-main       ClusterIP   10.106.233.220   <none>        9093/TCP                     2m2s
# alertmanager-operated   ClusterIP   None             <none>        9093/TCP,9094/TCP,9094/UDP   2m2s
# blackbox-exporter       ClusterIP   10.107.43.121    <none>        9115/TCP,19115/TCP           2m2s
# grafana                 NodePort    10.110.242.211   <none>        3000:30177/TCP               2m
# kube-state-metrics      ClusterIP   None             <none>        8443/TCP,9443/TCP            2m
# node-exporter           ClusterIP   None             <none>        9100/TCP                     2m
# prometheus-adapter      ClusterIP   10.106.206.99    <none>        443/TCP                      119s
# prometheus-k8s          ClusterIP   10.108.122.160   <none>        9090/TCP                     118s
# prometheus-operated     ClusterIP   None             <none>        9090/TCP,10901/TCP           118s
# prometheus-operator     ClusterIP   None             <none>        8443/TCP                     7m16s

kubectl get pods -n monitoring
# alertmanager-main-0                    2/2     Running   0          88m
# alertmanager-main-1                    2/2     Running   0          88m
# alertmanager-main-2                    2/2     Running   0          88m
# blackbox-exporter-556d889b47-x78hr     3/3     Running   0          88m
# grafana-674b67dc58-qkxbq               1/1     Running   0          88m
# kube-state-metrics-986b854-v57nq       3/3     Running   0          88m
# node-exporter-5htlx                    2/2     Running   0          88m
# node-exporter-m2fzn                    2/2     Running   0          88m
# node-exporter-zk254                    2/2     Running   0          88m
# prometheus-adapter-767f58977c-lnrmn    1/1     Running   0          88m
# prometheus-k8s-0                       3/3     Running   1          88m
# prometheus-k8s-1                       3/3     Running   1          88m
# prometheus-operator-59976dc7d5-pqqkd   2/2     Running   0          91m

kubectl top pods
# NAME                                        CPU(cores)   MEMORY(bytes)
# benedek-gapminderbokeh-report               0m           63Mi
# ....
# test-veo2                                   0m           1Mi

kubectl top nodes
# NAME             CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
# hpproliant       648m         8%     1406Mi          29%
# veo1         2988m        3%     24752Mi         4%
# veo2         1229m        1%     73913Mi         14%

# prometheus:
kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090
# amíg nincs ctrl-c megszakítva ssh -L 9999:127.0.0.1:9090 _veo böngészőből elérhető a prometheus

# grafana:
kubectl --namespace monitoring port-forward svc/grafana 3000
# admin/admin login után jelszóváltás:

# alert monitor
kubectl --namespace monitoring port-forward svc/alertmanager-main 9093
# műxik


# több konténeres pod esetnén a log olvasása:
kubectl logs -n monitoring alertmanager-main-2 -c config-reloader


##### ingress
kubectl create secret tls tls-monitoring -n monitoring \
  --cert=
  --key=

kubectl apply -f ingress.yaml


#HACK

kubectl apply -f grafana-claim.yaml
kubectl apply -f grafana-deployment.yaml

###
https://stackoverflow.com/questions/61121046/ingress-routing-rules-to-access-prometheus-server
# EZ SEM SEGÍT!!!!
#kubectl edit statefulsets.apps -n monitoring prometheus-k8s
#        - --web.route-prefix=/prometheus
#        - --web.external-url="https://monitoring.vo.elte.hu/prometheus"








# see also kubernetes-dashboard
https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/README.md
https://github.com/kubernetes/dashboard/issues/4179
dashboardban máris lehet listázni a dolgokat


## For embedding it into another site
https://github.com/grafana/grafana/issues/19729

# set to true if you host Grafana behind HTTPS. default is false.
cookie_secure = true
# set cookie SameSite attribute. defaults to `lax`. can be set to "lax", "strict" and "none"
cookie_samesite = none
# set to true if you want to allow browsers to render Grafana in a <frame>, <iframe>, <embed> or <object>. default is false.
allow_embedding = true

# OAUTH TO WORK add these envs my-kube-prometheus/manifests/grafana-deployment.yaml
        - name: "GF_SERVER_ROOT_URL"
        - name: "GF_SERVER_SERVE_FROM_SUB_PATH"
          value: "true"
        - name: "GF_SECURITY_COOKIE_SECURE"
          value: "true"
        - name: "GF_SECURITY_COOKIE_SAMESITE"
          value: "none"
        - name: "GF_SECURITY_ALLOW_EMBEDDING"
          value: "true"
        - name: "GF_AUTH_ANONYMOUS_ENABLED"
          value: "true"
        - name: "GF_AUTH_ANONYMOUS_ORG_NAME"
        - name: "GF_AUTH_BASIC_ENABLED"
          value: "false"
        - name: "GF_AUTH_OAUTH_AUTO_LOGIN"
          value: "true"
        - name: "GF_AUTH_GENERIC_OAUTH_ENABLED"
          value: "true"
        - name: "GF_AUTH_GENERIC_OAUTH_CLIENT_ID"
        - name: "GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET"
        - name: "GF_AUTH_GENERIC_OAUTH_AUTH_URL"
        - name: "GF_AUTH_GENERIC_OAUTH_TOKEN_URL"
        - name: "GF_AUTH_GENERIC_OAUTH_SCOPES"
        - name: "GF_AUTH_GENERIC_OAUTH_NAME"
        - name: "GF_AUTH_GENERIC_OAUTH_API_URL"
        - name: "GF_LOG_FILTERS"
          value: "oauth.generic_oauth:debug"
        - name: "GF_AUTH_PROXY_AUTO_SIGN_UP"
          value: "false"
        - name: "GF_AUTH_PROXY_ENABLED"
          value: "true"
